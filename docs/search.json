[
  {
    "objectID": "pixarmovies.html",
    "href": "pixarmovies.html",
    "title": "Pixar Films",
    "section": "",
    "text": "Show code\nlibrary(ggplot2)\nlibrary(tidytuesdayR)\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 10)\npublic_response &lt;- tuesdata$public_response\n\n\npublic_response |&gt; \n  ggplot(aes(x = rotten_tomatoes, y = metacritic)) +\n  geom_point(na.rm = TRUE) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\")\n\n\n\n\n\n\n\n\n\nShow code\n  labs(title= \"Rotten Tomatoes vs. Metacritic Scores for Pixar Movies\", \n       x = \"Rotten Tomatos score\", \n       y = \"Metacritic Score\")+\n  theme_classic()\n\n\nNULL\n\n\nSource:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Elene, and I’m an international student at Pomona College, where I’m a junior majoring in Cognitive Science with a concentration in Computer Science and Artificial Intelligence, and minoring in English. My academic background combines analytical thinking, technical problem-solving, and communication skills, which together shape how I approach data, strategy, and innovation.\n\n\nEducation\n\n\nPomona College, Claremont CA B.A. Cognitive Science (CS/AI), Minor in English, GPA 3.9 2023–2027\n\n\n Tbilisi Public School №199 of Physics and Mathematics, Tbilisi Math and Physics focus\n\n\n\n\nLanguages\n\n\n\nGeorgian, native\n\n\nEnglish, fluent\n\n\nRussian, fluent\n\n\nGerman, proficient\n\n\nSpanish, basic\n\n\n\n\n\nLinks\n\n\n\nDownload PDF\n\n\nGitHub"
  },
  {
    "objectID": "plot1.html",
    "href": "plot1.html",
    "title": "Gas Prices in the US",
    "section": "",
    "text": "Show code\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 26)\n\nweekly_gas_prices &lt;- tuesdata$weekly_gas_prices\nweekly_gas_prices &lt;-weekly_gas_prices |&gt; \n   mutate(year = year(ymd(date)))\n\n\nweekly_gas_prices |&gt; \n  ggplot(aes(x= year, y= price, color = fuel)) +\n  geom_smooth()+\n  labs(\n    title = \"Gas Price Trends Over Time\",\n    x = \"Year\",\n    y = \"Price (USD per gallon)\",\n    color = \"Gas Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSources:"
  },
  {
    "objectID": "bbcarticles.html",
    "href": "bbcarticles.html",
    "title": "The Rise of AI in BBC article titles",
    "section": "",
    "text": "Show code\nlibrary(RTextTools) \nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(data.table)\narticles &lt;- read.csv(\"bbc_news.csv\")\n\n\nFor the purpose of analysis, I compiled a list of AI-related terms to make the search process easier.\n\n\nShow code\nai_terms &lt;- \"(ai|artificial intelligence|machine learning|deep learning|neural network|chatgpt|openai|gpt[- ]?\\\\d|large language model|llm)\"\n\n\nThe titles in the dataset were converted to lowercase, and all AI-related terms from the original list were replaced with the placeholder “ai” to simplify analysis. In this context, “ai” represents any term referring to artificial intelligence (e.g., machine learning, chatgpt, neural networks). Additionally, the publication dates were converted from character strings to date variables for easier grouping.\n\n\nShow code\narticles &lt;- articles |&gt; \n  mutate(title = as.character(title),\n    lower_title = str_to_lower(title),\n    lower_title = str_replace_all(lower_title, ai_terms, \"ai\"),\n    topic_ai = str_detect(lower_title, \"\\\\bai\\\\b\"))\n\narticles &lt;- articles |&gt;\n  mutate(clean_date = str_replace(pubDate, \"^[A-Za-z]{3},\\\\s*\", \"\"),\n    clean_date = str_replace(clean_date, \"\\\\s*GMT$\", \"\"),\n    clean_date = dmy_hms(clean_date),\n    year_month = format(clean_date, \"%Y-%m\"))\n\n\n\n\nShow code\narticles_month &lt;- articles |&gt; \n  filter(topic_ai) |&gt; \n  group_by(year_month)|&gt; \n  summarize(number = n()) |&gt; \n  mutate(year_month = as.Date(paste0(year_month, \"-01\")))\n  \narticles_month |&gt; \n  ggplot(aes(x = year_month, y = number, color = number)) +\n  geom_line(size = 1.2)+\n  geom_point(color = \"#FFC107\", size = 2.5)+\n  scale_color_gradient(low = \"#FFF59D\", high = \"#E53935\")+\n  labs(title = \"AI Mentions by Month\",\n    subtitle = \"Tracking the rise of AI-related articles over time\",\n    x = NULL,\n    y = \"Number of AI Articles\") +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"5 months\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nShow code\nyear_month &lt;- articles |&gt;\n  mutate(yr = year(clean_date), mnth = month(clean_date)) |&gt;\n  group_by(yr, mnth) |&gt;\n  summarise(ai_mentions = sum(topic_ai))\n\nyear_month |&gt; \n  filter(yr&gt;2021) |&gt; \n  mutate(mnth = factor(mnth, levels = 1:12, labels = month.abb)) |&gt; \n  ggplot(aes(x = mnth, y = yr, fill = ai_mentions)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"#FFF59D\", high = \"#E53935\") +\n  labs(title = \"Heatmap of AI mentions by year and month\", x = \"Month\", y = \"Year\", fill = \"Count\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nThe line graph and the Heatmap show the number of BBC articles mentioning AI-related terms in their titles from May 2022 to January 2024. The data reveals an initial rise in mentions around April and May 2023, followed by a second peak in November 2023. The first surge corresponds to the public release of GPT-4 and the growing media attention surrounding generative AI tools such as ChatGPT and Microsoft Copilot. The later spike in November aligns with OpenAI’s Dev Day announcements and the coverage of the company’s leadership changes. Overall, 2023 had the highest number of AI-related articles in the dataset, indicating a significant rise of media attention.\n\n\nShow code\narticles_word &lt;- articles |&gt;\n  filter(topic_ai) |&gt; \n  mutate(ai_word = str_extract(lower_title, \"(?&lt;=ai )\\\\w+\")) |&gt; \n  filter(!is.na(ai_word)) |&gt; \n  group_by(ai_word) |&gt; \n  summarize(number_of_mentions = n()) |&gt; \n  select(ai_word, number_of_mentions) |&gt; \n  arrange(desc(number_of_mentions)) |&gt; \n  slice_head(n = 15)\n\narticles_adj &lt;- articles |&gt;\n  filter(topic_ai) |&gt; \n  mutate(ai_word = str_extract(lower_title, \"(?&lt;=ai is )\\\\w+\")) |&gt; \n  filter(!is.na(ai_word)) |&gt; \n  group_by(ai_word) |&gt; \n  summarize(number_of_mentions = n()) |&gt; \n  select(ai_word, number_of_mentions) |&gt; \n  arrange(desc(number_of_mentions))\n\n\n\n\nShow code\narticles_word |&gt; \n  ggplot(aes(x = ai_word, y = number_of_mentions)) +\n  geom_col(fill = \"forestgreen\") +\n  coord_flip() +\n  labs(\n    title = \"Most Common Words Following 'AI'\",\n    x = NULL,\n    y = \"Count\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nShow code\narticles_adj |&gt; \n  ggplot(aes(x = ai_word, y = number_of_mentions)) +\n  geom_col(fill = \"coral\") +\n  coord_flip() +\n  labs(\n    title = \"Most Common Words Following 'AI is'\",\n    x = NULL,\n    y = \"Count\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nTo explore how AI is used in context, I created two graphs showing the words that most frequently follow the placeholder term “ai”, and the words that follow the phrase “ai is”. I filtered the dataset to include only titles containing AI-related terms and used lookarounds to extract the surrounding words.\nThe first graph shows that the most common words following ai are “is,” “to,” “could,” and “in.” This pattern makes sense given the normal sentence structures in article titles, where ai often acts as a subject followed by verbs or prepositions.\nThe second graph focuses on words that appear after “ai is.” The most frequent one is “helping” (appearing twice), followed by single occurrences of words like “terrifying,” “not,” “most,” “matching,” “lying,” “hateful,” and “being.” It’s interesting that most of the adjectives carry a negative tone, reflecting the media narratives around AI, which emphasize fear, uncertainty, or ethical concerns over generative AI\nCitation:\nBBC News Articles. (n.d.). Kaggle. Retrieved October 8, 2025, from https://www.kaggle.com/datasets/bhavikjikadara/bbc-news-articles\nhttps://www.kaggle.com/datasets/bhavikjikadara/bbc-news-articles: R data source"
  },
  {
    "objectID": "permtest.html",
    "href": "permtest.html",
    "title": "Permutation Test",
    "section": "",
    "text": "The dataset, Students Data Analysis, contains information on students’ graduation acceptances (did not get into the graduate school, got into a local graduate school, got into a internatinoal graduate school) and their assigned class (class), a categorical variable with two groups. Students were initially divided into two classes, but instructors suspected that students in one class might be performing significantly differently from those in the other.\nTo test this hypothesis, I will use a permutation test to compare acceptance rates between the two classes. The procedure involved repeatedly shuffling the class labels while keeping the graduation scores fixed, then calculating the mean difference in graduation rates for each permutation. This generated a null distribution representing the differences we would expect if class assignment had no real effect on performance.\n\nHypotheses\nNull hypothesis (H₀): There is no difference in graduate school acceptance rates between the two classes. Any observed difference is due to random chance.\nAlternative hypothesis (Hₐ): There is a difference in graduate school acceptance rates between the two classes.\nLoading the dataset and packages:\n\n\nShow code\nlibrary(RTextTools) \nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(gt)\nstudents &lt;- read.csv(\"Students Data.csv\")\n\n\n\n\nStep 1: Explore and Summarize the Dataset\nBecause the dataset includes two types of graduate school acceptances (international and local) I combined them into a single category and calculated the average graduation acceptance rate.\n\n\nShow code\nstudents_clean &lt;- students |&gt; \n  mutate(grad = as.integer(y&gt;0)) |&gt; \n  select(class, grad) \n\nstudents_clean |&gt; \n  group_by(class) |&gt; \n  summarize(ave_grad = mean(grad)) |&gt; \n  gt() |&gt; \n  tab_header(\n    title = \"Average Grad School Acceptances by Class\"\n  ) \n\n\n\n\n\n\n\n\nAverage Grad School Acceptances by Class\n\n\nclass\nave_grad\n\n\n\n\nA\n0.5131579\n\n\nB\n0.3793103\n\n\n\n\n\n\n\nShow code\nstudents_clean |&gt; \n  group_by(class) |&gt; \n  summarize(ave_grad = mean(grad)) |&gt; \n  summarize(ave_diff =diff(ave_grad)) |&gt; \n  gt() |&gt;\n  tab_header(\n    title = \"Differences in Grad School Acceptances Between Classes\"\n  )\n\n\n\n\n\n\n\n\nDifferences in Grad School Acceptances Between Classes\n\n\nave_diff\n\n\n\n\n-0.1338475\n\n\n\n\n\n\n\nFrom the analysis, we can see that Class A had a higher graduate school acceptance rate than Class B.\n\n\nStep 2: Permutation Test Function\nThis function tests whether the difference in average grad school accentance rates between the two classes could occur by chance. It works by shuffling the grad values, then calculating the average graduate school acceptance rate for each class in both the observed and permuted data. The function returns the difference between these averages (ave_diff and perm_diff). I set a random seed for reproducibility and used the map() function to run the function 1,000 times, combining all results into a single data frame called perm_stats.\n\n\nShow code\ngrad_diff &lt;- function(rep, students_clean) {\n  students_clean |&gt; \n    mutate(grad_perm = sample(grad, replace = FALSE)) |&gt; \n    group_by(class) |&gt; \n    summarize(obs_ave = mean(grad),\n            perm_ave = mean(grad_perm)) |&gt; \n    summarize(ave_diff =diff(obs_ave), \n            perm_diff = diff(perm_ave), \n            rep = rep)\n}\n\n\nset.seed(47)\nnumber &lt;- 1000\nperm_stats&lt;- map(c(1:number), grad_diff, students_clean) |&gt; \n  list_rbind()\n\n\n\n\nStep 3: Visualize the Null Distribution\nThe histogram below displays this null distribution of permuted mean differences (perm_diff), with the red line showing the observed mean difference from the original data (ave_diff).\n\n\nShow code\nperm_stats |&gt; \n  ggplot(aes(x = perm_diff))+\n  geom_histogram(bins  = 40 ,fill = \"skyblue\", color = \"white\", alpha = 0.8) +\n  labs(\n    title = \"Permutation Distribution of Mean Differences\",\n    subtitle = \"Red line = observed difference\",\n    x = \"Permutation Mean Difference\",\n    y = \"Count\"\n  ) +\n  geom_vline(aes(xintercept = ave_diff), color = \"red\", linewidth = 1.2)+\n  theme_classic()+\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nFrom the plot, we can see that the red line, which represents the observed difference, is slightly to one side of the null distribution. However, it does not seem far enough from the center to suggest a meaningful or statistically significant difference at first glance.\n\n\nStep 4: Calculate the p-value\nFinally, I calculated the p-value\n\n\nShow code\nperm_stats |&gt; \n  summarize(p_val_ave = mean(perm_diff &lt; ave_diff)) |&gt; \n  gt() |&gt; \n  tab_header(\n    title = \"Left-tailed p-value\"\n  )\n\n\n\n\n\n\n\n\nLeft-tailed p-value\n\n\np_val_ave\n\n\n\n\n0.085\n\n\n\n\n\n\n\n\n\nStep 5: Results\nThe results of the permutation test show that the p-value is bigger than 0.05. This means the observed difference in graduate school acceptance rates between the two classes is not statistically significant. In other words, the difference we see might have occurred by random chance, so we fail to reject the null hypothesis (H₀). Based on this analysis, there isn’t enough evidence to conclude that class assignment had a meaningful effect on graduate school acceptance outcomes.\n\n\nCitation:\nStudents Data Analysis. (n.d.). Kaggle. Retrieved October 28, 2025, from https://www.kaggle.com/datasets/erqizhou/students-data-analysis\nhttps://www.kaggle.com/datasets/erqizhou/students-data-analysis: R data source"
  }
]